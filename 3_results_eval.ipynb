{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results evaluation\n",
    "by: Kaike Wesley Reis\n",
    "\n",
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard modules\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Graphical modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Bootstrap\n",
    "from sklearn.utils import resample\n",
    "# Evaluation\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report, confusion_matrix, brier_score_loss\n",
    "from sklearn.metrics import f1_score,roc_auc_score,recall_score, precision_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# Import models\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "mdl_lre = joblib.load('results_modelsDevelopment/model_lre_oversampled.sav')\n",
    "mdl_svm = joblib.load('results_modelsDevelopment/model_svm_oversampled.sav')\n",
    "mdl_rfc = joblib.load('results_modelsDevelopment/model_rfc_oversampled.sav')\n",
    "mdl_bst = joblib.load('results_modelsDevelopment/model_bst_oversampled.sav')\n",
    "mdl_xgb = joblib.load('results_modelsDevelopment/model_xgb_oversampled.sav')\n",
    "# Dummy models\n",
    "mdl_dmf = joblib.load('results_modelsDevelopment/model_dummy_mf_oversampled.sav')\n",
    "mdl_dst = joblib.load('results_modelsDevelopment/model_dummy_st_oversampled.sav')\n",
    "# Testset\n",
    "x_test = pd.read_csv('results_modelsDevelopment/x_test.csv')\n",
    "y_test = pd.read_csv('results_modelsDevelopment/y_test.csv')\n",
    "# BOCV-5 results\n",
    "bocv5_results = pd.read_csv('results_modelsDevelopment/BO5CV_best_results.csv')\n",
    "# Backtest\n",
    "x_resp = pd.read_csv('results_modelsDevelopment/x_resp.csv')\n",
    "y_resp = pd.Series(np.zeros((len(x_resp),)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxilar functions & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap parameters\n",
    "REPETITIONS = 999\n",
    "RS_GENERATOR = range(0,REPETITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, x_test, y_true):\n",
    "    # Generate a prediction using the model\n",
    "    y_pred = model.predict(x_test)\n",
    "    # Calculate metrics\n",
    "    f1s = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    # return results\n",
    "    return {'F1-Score':round(100*f1s,3),'AUC ROC':round(100*auc,3),'Accuracy':round(100*acc,3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix_results(model, x_test, y_true):\n",
    "    # Generate a prediction using the model\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Calculate a confusion matrix to retrieve the binary CM values\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Return\n",
    "    return {'TN':tn,'TP':tp,'FN':fn,'FP':fp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_1_minus_brier_score_loss(model, x_test, y_true):\n",
    "    y_pred = model.predict_proba(x_test)[:,1]\n",
    "    prob_score = brier_score_loss(y_true, y_pred, pos_label=1)\n",
    "    return {'Brier Score':round(prob_score, 3), '1 - Brier Score':round(1-prob_score, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_resampling(x_test, y_test, rs_number, sample_size):\n",
    "    # Generate X sample\n",
    "    bootstrap_x = resample(x_test, replace=True, n_samples=sample_size, random_state=rs_number)\n",
    "    # Get index for X to get Y value\n",
    "    bootstrap_y = y_test.loc[bootstrap_x.index]\n",
    "    # Return\n",
    "    return bootstrap_x, bootstrap_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_confidence_interval(values):\n",
    "    percents = np.percentile(values, [2.5, 97.5])\n",
    "    lower_bound = round(max(0.0, percents[0]), 3)\n",
    "    upper_bound = round(min(1.0, percents[1]), 3)\n",
    "    mean_value = round(np.mean(values), 3)\n",
    "    return (lower_bound, mean_value, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_best_model(model, x_test, y_true):\n",
    "    # Generate a prediction using the model\n",
    "    y_pred = model.predict(x_test)\n",
    "       \n",
    "    # Calculate several metrics with SKlearn\n",
    "    f1score = f1_score(y_true, y_pred) #\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred,pos_label=1)\n",
    "    specificity = recall_score(y_true, y_pred,pos_label=0)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "\n",
    "    # Return\n",
    "    return f1score, roc_auc, sensitivity, specificity, precision  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_backtest_accuracy(model, x_resp, y_resp):\n",
    "    # Generate a prediction using the model\n",
    "    y_pred = model.predict(x_resp)\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_resp, y_pred)\n",
    "    # return\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_backtest_real_accuracy(mdl_object, x_resp, y_resp):\n",
    "    return {'ACC (%)':100*accuracy_score(y_resp.values.ravel(),mdl_object.predict(x_resp)).round(2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_using_bootstrap_best_model(model, x_test, y_test, rs_generator):\n",
    "    # Metrics list\n",
    "    f1s_list = []\n",
    "    roc_list = []\n",
    "    sen_list = []\n",
    "    spe_list = []\n",
    "    pre_list = []\n",
    "    \n",
    "    # Bootstrap Stratified\n",
    "    # Get indexes for positive and negative cases\n",
    "    neg_idx = y_test.loc[y_test['COVID-19 Exam result'] == 0, 'COVID-19 Exam result'].index\n",
    "    pos_idx = y_test.loc[y_test['COVID-19 Exam result'] == 1, 'COVID-19 Exam result'].index\n",
    "    \n",
    "    # Split X set for positive and negative cases\n",
    "    x_test_neg = x_test.loc[neg_idx,:]\n",
    "    x_test_pos = x_test.loc[pos_idx,:]\n",
    "    \n",
    "    # Split Y set for positive and negative cases\n",
    "    y_test_neg = y_test.loc[neg_idx,:]\n",
    "    y_test_pos = y_test.loc[pos_idx,:]\n",
    "    \n",
    "    # Loop to generate a sample and generate metrics\n",
    "    for rs in rs_generator:\n",
    "        # Bootstrap resampling - negative\n",
    "        x_sample_neg, y_sample_neg = bootstrap_resampling(x_test_neg, y_test_neg, rs_number=rs, sample_size=len(y_test_neg))\n",
    "        # Bootstrap resampling - positive\n",
    "        x_sample_pos, y_sample_pos = bootstrap_resampling(x_test_pos, y_test_pos, rs_number=rs, sample_size=len(y_test_pos))\n",
    "        # Merge them into one\n",
    "        x_sample = x_sample_neg.append(x_sample_pos)\n",
    "        y_sample = y_sample_neg.append(y_sample_pos)\n",
    "        # Calculate the metrics\n",
    "        f1s, roc, sen, spe, pre = calculate_metrics_best_model(model, x_sample, y_sample)\n",
    "        # Append results\n",
    "        f1s_list.append(f1s)\n",
    "        roc_list.append(roc)\n",
    "        sen_list.append(sen)\n",
    "        spe_list.append(spe)\n",
    "        pre_list.append(pre)\n",
    "    \n",
    "    # Return\n",
    "    return {'F1S':f1s_list,'ROC':roc_list,'SEN':sen_list,'SPE':spe_list,'PRE':pre_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_using_bootstrap_for_backtest(model, x_resp, y_resp, rs_generator):\n",
    "    # Metrics list\n",
    "    acc_list = []\n",
    "    \n",
    "    # Loop to generate a sample and generate metrics\n",
    "    for rs in rs_generator:\n",
    "        x_sample, y_sample = bootstrap_resampling(x_resp, y_resp, rs_number=rs)\n",
    "        acc = calculate_backtest_accuracy(model, x_sample, y_sample)\n",
    "        # Append results\n",
    "        acc_list.append(acc)\n",
    "    \n",
    "    # Return\n",
    "    return acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pvalue(pval):\n",
    "    if pval > 0.05:\n",
    "        return 'No significant difference between distributions (fail to reject H0)'\n",
    "    else:\n",
    "        return 'Different distributions (reject H0)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION 0 - Model Development results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>ai_algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.442</td>\n",
       "      <td>3.080</td>\n",
       "      <td>lre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94.785</td>\n",
       "      <td>1.352</td>\n",
       "      <td>svm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94.561</td>\n",
       "      <td>2.021</td>\n",
       "      <td>rfc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96.024</td>\n",
       "      <td>1.987</td>\n",
       "      <td>bst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93.683</td>\n",
       "      <td>2.448</td>\n",
       "      <td>xgb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score ai_algorithm\n",
       "0           83.442           3.080          lre\n",
       "1           94.785           1.352          svm\n",
       "2           94.561           2.021          rfc\n",
       "3           96.024           1.987          bst\n",
       "4           93.683           2.448          xgb"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rounding up\n",
    "bocv5_results['mean_test_score'] = np.round(100*bocv5_results['mean_test_score'],3)\n",
    "bocv5_results['std_test_score'] = np.round(100*bocv5_results['std_test_score'],3)\n",
    "\n",
    "# Show\n",
    "bocv5_results[['mean_test_score','std_test_score','ai_algorithm']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION 1 - Calculate several metrics for test set \n",
    "\n",
    "**Metrics**\n",
    "- F1 Score (related to precision and recall)\n",
    "- AUC ROC Score\n",
    "- Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.649</td>\n",
       "      <td>77.244</td>\n",
       "      <td>78.889</td>\n",
       "      <td>LRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.852</td>\n",
       "      <td>74.038</td>\n",
       "      <td>85.556</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.261</td>\n",
       "      <td>86.218</td>\n",
       "      <td>94.444</td>\n",
       "      <td>RFC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57.143</td>\n",
       "      <td>73.077</td>\n",
       "      <td>90.000</td>\n",
       "      <td>BST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.333</td>\n",
       "      <td>75.962</td>\n",
       "      <td>88.889</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17.857</td>\n",
       "      <td>45.833</td>\n",
       "      <td>48.889</td>\n",
       "      <td>DST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   F1-Score  AUC ROC  Accuracy Model\n",
       "0    48.649   77.244    78.889   LRE\n",
       "1    51.852   74.038    85.556   SVM\n",
       "2    78.261   86.218    94.444   RFC\n",
       "3    57.143   73.077    90.000   BST\n",
       "4    58.333   75.962    88.889   XGB\n",
       "5    17.857   45.833    48.889   DST"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List to keep our results\n",
    "test_results = []\n",
    "# Loop to calculate test results\n",
    "for mdl_object, mdl_name in zip([mdl_lre,mdl_svm,mdl_rfc,mdl_bst,mdl_xgb,mdl_dst],['LRE','SVM','RFC','BST','XGB','DST']):\n",
    "    test_result = calculate_metrics(mdl_object, x_test, y_test)\n",
    "    test_result['Model'] = mdl_name\n",
    "    test_results.append(test_result)\n",
    "\n",
    "# Transform into a dataframe\n",
    "df_test_results = pd.DataFrame(test_results)\n",
    "df_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION 2 - Get Confusion Matrix results from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>LRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RFC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>BST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>DST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TN  TP  FN  FP Model\n",
       "0  62   9   3  16   LRE\n",
       "1  70   7   5   8   SVM\n",
       "2  76   9   3   2   RFC\n",
       "3  75   6   6   3   BST\n",
       "4  73   7   5   5   XGB\n",
       "5  40   5   7  38   DST"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List to keep our CM results\n",
    "test_results = []\n",
    "# Loop to calculate CM results\n",
    "for mdl_object, mdl_name in zip([mdl_lre,mdl_svm,mdl_rfc,mdl_bst,mdl_xgb,mdl_dst],['LRE','SVM','RFC','BST','XGB','DST']):\n",
    "    test_result = calculate_confusion_matrix_results(mdl_object, x_test, y_test)\n",
    "    test_result['Model'] = mdl_name\n",
    "    test_results.append(test_result)\n",
    "\n",
    "# Transform into a dataframe\n",
    "df_cm_results = pd.DataFrame(test_results)\n",
    "df_cm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION 3 - Get a probabilistic metric approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brier Score</th>\n",
       "      <th>1 - Brier Score</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.861</td>\n",
       "      <td>LRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.104</td>\n",
       "      <td>0.896</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.073</td>\n",
       "      <td>0.927</td>\n",
       "      <td>RFC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.091</td>\n",
       "      <td>0.909</td>\n",
       "      <td>BST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.081</td>\n",
       "      <td>0.919</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.478</td>\n",
       "      <td>0.522</td>\n",
       "      <td>DST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Brier Score  1 - Brier Score Model\n",
       "0        0.139            0.861   LRE\n",
       "1        0.104            0.896   SVM\n",
       "2        0.073            0.927   RFC\n",
       "3        0.091            0.909   BST\n",
       "4        0.081            0.919   XGB\n",
       "5        0.478            0.522   DST"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List to keep our CM results\n",
    "test_results = []\n",
    "# Loop to calculate CM results\n",
    "for mdl_object, mdl_name in zip([mdl_lre,mdl_svm,mdl_rfc,mdl_bst,mdl_xgb,mdl_dst],['LRE','SVM','RFC','BST','XGB','DST']):\n",
    "    test_result = calculate_1_minus_brier_score_loss(mdl_object, x_test, y_test)\n",
    "    test_result['Model'] = mdl_name\n",
    "    test_results.append(test_result)\n",
    "\n",
    "# Transform into a dataframe\n",
    "df_brier_results = pd.DataFrame(test_results)\n",
    "df_brier_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION 5 - Backtests Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC (%)</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85.0</td>\n",
       "      <td>LRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.0</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.0</td>\n",
       "      <td>RFC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96.0</td>\n",
       "      <td>BST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96.0</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.0</td>\n",
       "      <td>DST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ACC (%) Model\n",
       "0     85.0   LRE\n",
       "1     85.0   SVM\n",
       "2     92.0   RFC\n",
       "3     96.0   BST\n",
       "4     96.0   XGB\n",
       "5     42.0   DST"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List to keep our CM results\n",
    "test_results = []\n",
    "# Loop to calculate CM results\n",
    "for mdl_object, mdl_name in zip([mdl_lre,mdl_svm,mdl_rfc,mdl_bst,mdl_xgb,mdl_dst],['LRE','SVM','RFC','BST','XGB','DST']):\n",
    "    test_result = calculate_backtest_real_accuracy(mdl_object, x_resp, y_resp)\n",
    "    test_result['Model'] = mdl_name\n",
    "    test_results.append(test_result)\n",
    "\n",
    "# Transform into a dataframe\n",
    "df_backtest_results = pd.DataFrame(test_results)\n",
    "df_backtest_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Commentary over previous results\n",
    "As you can see **Random Forest** performs better compared to the others (considering the best results into CM/Metrics/Brier Score):\n",
    "- Metrics: Best F1-Score and AUC\n",
    "- CM: Best TN/TP/FP/FN\n",
    "- Best Brier Score\n",
    "- Lose by one missclassified sample\n",
    "\n",
    "The only result where RF was not the best, by one sample, was backtest evaluation. So the selected model will be the RF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL RESULT - Bootstrap CI 95% for selected AI algorithm with several metrics\n",
    "- F1-Score\n",
    "- AUC ROC\n",
    "- Sensitivity\n",
    "- Specificity\n",
    "- Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get BCI 95% for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of metrics using Bootstrap\n",
    "selected_model_bci_results = get_metrics_using_bootstrap_best_model(mdl_rfc, x_test, y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe result\n",
    "df_bci_best_model = pd.DataFrame(index=[0],columns=list(selected_model_bci_results.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1S</th>\n",
       "      <th>ROC</th>\n",
       "      <th>SEN</th>\n",
       "      <th>SPE</th>\n",
       "      <th>PRE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.571, 0.777, 0.923)</td>\n",
       "      <td>(0.737, 0.859, 0.959)</td>\n",
       "      <td>(0.5, 0.744, 0.921)</td>\n",
       "      <td>(0.936, 0.975, 1.0)</td>\n",
       "      <td>(0.615, 0.83, 1.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     F1S                    ROC                  SEN  \\\n",
       "0  (0.571, 0.777, 0.923)  (0.737, 0.859, 0.959)  (0.5, 0.744, 0.921)   \n",
       "\n",
       "                   SPE                 PRE  \n",
       "0  (0.936, 0.975, 1.0)  (0.615, 0.83, 1.0)  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate BCI95%\n",
    "for i in df_bci_best_model.columns:\n",
    "    df_bci_best_model.loc[0, i] = bootstrap_confidence_interval(selected_model_bci_results[i])\n",
    "\n",
    "# Show results\n",
    "df_bci_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.261</td>\n",
       "      <td>86.218</td>\n",
       "      <td>94.444</td>\n",
       "      <td>RFC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   F1-Score  AUC ROC  Accuracy Model\n",
       "2    78.261   86.218    94.444   RFC"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Evaluation 1 results for F1S and ROC\n",
    "df_test_results.loc[df_test_results['Model'] == 'RFC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Export results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_results.to_csv('results_resultsEval/eval_1_testSet_metrics.csv',index=False)\n",
    "df_cm_results.to_csv('results_resultsEval/eval_2_confusionMatrix_metrics.csv',index=False)\n",
    "df_brier_results.to_csv('results_resultsEval/eval_3_testSet_brierscore.csv',index=False)\n",
    "df_backtest_results.to_csv('results_resultsEval/eval_4_backtest.csv',index=False)\n",
    "df_bci_best_model.to_csv('results_resultsEval/best_model_bci_result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
