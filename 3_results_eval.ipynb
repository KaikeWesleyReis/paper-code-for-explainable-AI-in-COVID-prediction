{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results evaluation\n",
    "by: Kaike Wesley Reis\n",
    "\n",
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard modules\n",
    "from time import sleep\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Graphical modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Bootstrap\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Evaluation\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score,roc_auc_score,recall_score, precision_score, accuracy_score\n",
    "\n",
    "# Import models\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models - Imbalanced\n",
    "lre_imb = joblib.load('results_modelsDevelopment/lre_imbalanced.sav')\n",
    "rfc_imb = joblib.load('results_modelsDevelopment/rfc_imbalanced.sav')\n",
    "svm_imb = joblib.load('results_modelsDevelopment/svm_imbalanced.sav')\n",
    "bst_imb = joblib.load('results_modelsDevelopment/bst_imbalanced.sav')\n",
    "xgb_imb = joblib.load('results_modelsDevelopment/xgb_imbalanced.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy models - Imbalanced\n",
    "dmf_imb = joblib.load('results_modelsDevelopment/dummy_mf_imbalanced.sav')\n",
    "dst_imb = joblib.load('results_modelsDevelopment/dummy_st_imbalanced.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models - Oversampled\n",
    "lre_ove = joblib.load('results_modelsDevelopment/lre_oversampled.sav')\n",
    "rfc_ove = joblib.load('results_modelsDevelopment/rfc_oversampled.sav')\n",
    "svm_ove = joblib.load('results_modelsDevelopment/svm_oversampled.sav')\n",
    "bst_ove = joblib.load('results_modelsDevelopment/bst_oversampled.sav')\n",
    "xgb_ove = joblib.load('results_modelsDevelopment/xgb_oversampled.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy models - Oversampled\n",
    "dmf_ove = joblib.load('results_modelsDevelopment/dummy_mf_oversampled.sav')\n",
    "dst_ove = joblib.load('results_modelsDevelopment/dummy_st_oversampled.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testset\n",
    "x_test = pd.read_csv('results_modelsDevelopment/x_test.csv')\n",
    "y_test = pd.read_csv('results_modelsDevelopment/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest\n",
    "x_resp = pd.read_csv('results_modelsDevelopment/x_resp.csv')\n",
    "y_resp = np.zeros((len(x_resp),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Calculate Several Metrics using bootstrap resampling for all ML models**\n",
    "\n",
    "**Metrics**\n",
    "- F1 Score (related to precision and recall)\n",
    "- AUC ROC Score\n",
    "\n",
    "\n",
    "## FUNCTION - Bootstrap Sampling\n",
    "[one of the source for implementation](https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/)\n",
    "\n",
    "Info:\n",
    "- sampling with replacement\n",
    "- Sample size: same size of test set\n",
    "- Repetition: 2000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables fixed for bootstrap\n",
    "REPETITIONS = 2000\n",
    "SAMPLE_SIZE = len(y_test)\n",
    "RS_GENERATOR = range(0,REPETITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_resampling(x_testset, y_testset, rs_number, sample_size=SAMPLE_SIZE):\n",
    "    # Generate X sample\n",
    "    bootstrap_x = resample(x_testset, replace=True, n_samples=sample_size, random_state=rs_number)\n",
    "    # Get index for X to get Y value\n",
    "    bootstrap_y = y_testset.loc[bootstrap_x.index]\n",
    "    # Return\n",
    "    return bootstrap_x, bootstrap_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION - Calculate Metrics Specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, x_sample, y_sample_true):\n",
    "    # Generate a prediction using the model\n",
    "    y_sample_pred = model.predict(x_sample)\n",
    "    # Calculate several metrics with SKlearn\n",
    "    f1score = f1_score(y_sample_true, y_sample_pred)\n",
    "    roc_auc = roc_auc_score(y_sample_true, y_sample_pred)\n",
    "    # Return\n",
    "    return f1score,roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION - Calculate metrics using bootstramp sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_using_bootstrap(model, x_testset, y_testset, rs_generator):\n",
    "    # Metrics list\n",
    "    f1s_list = []\n",
    "    roc_list = []\n",
    "    \n",
    "    # Loop to generate a sample and generate metrics\n",
    "    for rs in rs_generator:\n",
    "        x_sample, y_sample = bootstrap_resampling(x_testset, y_testset, rs_number=rs)\n",
    "        f1s,roc = calculate_metrics(model, x_sample=x_sample, y_sample_true=y_sample)\n",
    "        # Append results\n",
    "        f1s_list.append(f1s)\n",
    "        roc_list.append(roc)\n",
    "    \n",
    "    # Return\n",
    "    return f1s_list, roc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting results ...\n",
    "\n",
    "### Imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre_imb_results = get_metrics_using_bootstrap(model=lre_imb, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_imb_results = get_metrics_using_bootstrap(model=svm_imb, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_imb_results = get_metrics_using_bootstrap(model=rfc_imb, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_imb_results = get_metrics_using_bootstrap(model=bst_imb, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_imb_results = get_metrics_using_bootstrap(model=xgb_imb, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmf_imb_results = get_metrics_using_bootstrap(model=dmf_imb, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_imb_results = get_metrics_using_bootstrap(model=dst_imb, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre_ove_results = get_metrics_using_bootstrap(model=lre_ove, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_ove_results = get_metrics_using_bootstrap(model=svm_ove, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_ove_results = get_metrics_using_bootstrap(model=rfc_ove, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_ove_results = get_metrics_using_bootstrap(model=bst_ove, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_ove_results = get_metrics_using_bootstrap(model=xgb_ove, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmf_ove_results = get_metrics_using_bootstrap(model=dmf_ove, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_ove_results = get_metrics_using_bootstrap(model=dst_ove, x_testset=x_test, y_testset=y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap 95% CI\n",
    "\n",
    "## FUNCTION - Bootstrap CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_confidence_interval(values):\n",
    "    percents = np.percentile(values, [2.5, 97.5])\n",
    "    lower_bound = round(max(0.0, percents[0]), 3)\n",
    "    upper_bound = round(min(1.0, percents[1]), 3)\n",
    "    mean_value = round(np.mean(values), 3)\n",
    "    return (lower_bound, mean_value, upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate 95% Bootstrap CI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set dataframes with answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Def index\n",
    "idx_models = ['Logistic Regression','SVM','Random Forest','Gradient Boosting','XGBoost',\n",
    "              'Dummy Most Frequent', 'Dummy Stratified']\n",
    "# Def dataframes results\n",
    "df_ci_imb = pd.DataFrame(index=idx_models, columns=metrics_names)\n",
    "df_ci_ove = pd.DataFrame(index=idx_models, columns=metrics_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------- Imbalanced ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>AUC ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>(0.333, 0.552, 0.737)</td>\n",
       "      <td>(0.698, 0.829, 0.927)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>(0.294, 0.509, 0.7)</td>\n",
       "      <td>(0.671, 0.809, 0.913)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>(0.235, 0.511, 0.741)</td>\n",
       "      <td>(0.568, 0.719, 0.867)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>(0.2, 0.512, 0.759)</td>\n",
       "      <td>(0.558, 0.695, 0.844)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>(0.111, 0.388, 0.643)</td>\n",
       "      <td>(0.51, 0.641, 0.786)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dummy Most Frequent</th>\n",
       "      <td>(0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.5, 0.5, 0.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dummy Stratified</th>\n",
       "      <td>(0.0, 0.128, 0.316)</td>\n",
       "      <td>(0.407, 0.498, 0.613)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  F1-Score                AUC ROC\n",
       "Logistic Regression  (0.333, 0.552, 0.737)  (0.698, 0.829, 0.927)\n",
       "SVM                    (0.294, 0.509, 0.7)  (0.671, 0.809, 0.913)\n",
       "Random Forest        (0.235, 0.511, 0.741)  (0.568, 0.719, 0.867)\n",
       "Gradient Boosting      (0.2, 0.512, 0.759)  (0.558, 0.695, 0.844)\n",
       "XGBoost              (0.111, 0.388, 0.643)   (0.51, 0.641, 0.786)\n",
       "Dummy Most Frequent        (0.0, 0.0, 0.0)        (0.5, 0.5, 0.5)\n",
       "Dummy Stratified       (0.0, 0.128, 0.316)  (0.407, 0.498, 0.613)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Auxiliar variables\n",
    "data = df_ci_imb\n",
    "models_results = [lre_imb_results,svm_imb_results,rfc_imb_results,bst_imb_results,xgb_imb_results,\n",
    "                  dmf_imb_results,dst_imb_results]\n",
    "# Loop\n",
    "for model_results, model_name in zip(models_results, data.index): \n",
    "    for metric_name, idx in zip(metrics_names, range(0, len(metrics_names))):\n",
    "        data.loc[model_name, metric_name] = bootstrap_confidence_interval(model_results[idx])\n",
    "# Show results\n",
    "df_ci_imb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Using **Dummy classifiers** as baseline standard, ML models that present a CI with any intersection for the F1-Score and AUC ROC metrics will be eliminated from the analysis.\n",
    "\n",
    "**Most Frequent**: Considering this one, none of the models can be disqualified, but XGBoost could be very close at AUC ROC.\n",
    "\n",
    "**Stratified**: Looking F1-Score all the models, except Logistic Regression (note that this occurred with a small difference of 0.017), can be disqualified. Looking AUC ROC, we can keep Logistic Regression and SVM.\n",
    "\n",
    "**Keeped models**: For imbalanced dataset, Logistic Regression is the only qualified given such criterious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------- Oversampled ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>AUC ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>(0.263, 0.482, 0.684)</td>\n",
       "      <td>(0.628, 0.774, 0.906)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>(0.24, 0.508, 0.727)</td>\n",
       "      <td>(0.581, 0.739, 0.878)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>(0.533, 0.774, 0.941)</td>\n",
       "      <td>(0.719, 0.862, 0.988)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>(0.19, 0.488, 0.741)</td>\n",
       "      <td>(0.547, 0.69, 0.84)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>(0.333, 0.626, 0.833)</td>\n",
       "      <td>(0.615, 0.773, 0.917)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dummy Most Frequent</th>\n",
       "      <td>(0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.5, 0.5, 0.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dummy Stratified</th>\n",
       "      <td>(0.073, 0.21, 0.357)</td>\n",
       "      <td>(0.344, 0.501, 0.654)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  F1-Score                AUC ROC\n",
       "Logistic Regression  (0.263, 0.482, 0.684)  (0.628, 0.774, 0.906)\n",
       "SVM                   (0.24, 0.508, 0.727)  (0.581, 0.739, 0.878)\n",
       "Random Forest        (0.533, 0.774, 0.941)  (0.719, 0.862, 0.988)\n",
       "Gradient Boosting     (0.19, 0.488, 0.741)    (0.547, 0.69, 0.84)\n",
       "XGBoost              (0.333, 0.626, 0.833)  (0.615, 0.773, 0.917)\n",
       "Dummy Most Frequent        (0.0, 0.0, 0.0)        (0.5, 0.5, 0.5)\n",
       "Dummy Stratified      (0.073, 0.21, 0.357)  (0.344, 0.501, 0.654)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Auxiliar variables\n",
    "data = df_ci_ove\n",
    "models_results = [lre_ove_results,svm_ove_results,rfc_ove_results,bst_ove_results,xgb_ove_results,\n",
    "                  dmf_ove_results,dst_ove_results]\n",
    "# Loop\n",
    "for model_results, model_name in zip(models_results, data.index): \n",
    "    for metric_name, idx in zip(metrics_names, range(0, len(metrics_names))):\n",
    "        data.loc[model_name, metric_name] = bootstrap_confidence_interval(model_results[idx])\n",
    "# Show results\n",
    "df_ci_ove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Using **Dummy classifiers** as baseline standard, ML models that present a CI with any intersection for the F1-Score and AUC ROC metrics will be eliminated from the analysis.\n",
    "\n",
    "**Most Frequent**: Considering this one, none of the models can be disqualified.\n",
    "\n",
    "**Stratified**: Looking F1-Score all the models, except Random Forest, can be disqualified. Looking AUC ROC, the same occurs: only Random Forest pass in that criteria.\n",
    "\n",
    "**Keeped models**: For oversampled dataset, Random Forest is the only qualified given such criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix elements on Test set\n",
    "\n",
    "## Set answers dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Def index\n",
    "idx_models = ['Logistic Regression','SVM','Random Forest','Gradient Boosting','XGBoost',\n",
    "              'Dummy Most Frequent', 'Dummy Stratified']\n",
    "col_elements = ['TP','FP','TN','FN']\n",
    "\n",
    "# Def dataframes results\n",
    "df_cm_imb = pd.DataFrame(index=idx_models, columns=col_elements)\n",
    "df_cm_ove = pd.DataFrame(index=idx_models, columns=col_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(model, xTest, yTest):\n",
    "    # predict\n",
    "    yPred = model.predict(xTest)\n",
    "    # Get elements from cm\n",
    "    tn, fp, fn, tp = confusion_matrix(yTest, yPred).ravel()\n",
    "    # return\n",
    "    return tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------- Imbalanced ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>76</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dummy Most Frequent</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dummy Stratified</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     TP  FP  TN  FN\n",
       "Logistic Regression  10  14  64   2\n",
       "SVM                  10  17  61   2\n",
       "Random Forest         6   5  73   6\n",
       "Gradient Boosting     5   2  76   7\n",
       "XGBoost               4   4  74   8\n",
       "Dummy Most Frequent   0   0  78  12\n",
       "Dummy Stratified      3  12  66   9"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aux\n",
    "data = df_cm_imb\n",
    "models_objects = [lre_imb,svm_imb,rfc_imb,bst_imb,xgb_imb,dmf_imb,dst_imb]\n",
    "# Loop\n",
    "for model_name, models_object in zip(idx_models, models_objects):\n",
    "    tn, fp, fn, tp = generate_report(models_object,x_test, y_test)\n",
    "    data.loc[model_name, 'TP'] = tp\n",
    "    data.loc[model_name, 'FP'] = fp\n",
    "    data.loc[model_name, 'TN'] = tn\n",
    "    data.loc[model_name, 'FN'] = fn\n",
    "# Show results\n",
    "df_cm_imb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------- Oversampled ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dummy Most Frequent</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dummy Stratified</th>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    TP  FP  TN  FN\n",
       "Logistic Regression  9  16  62   3\n",
       "SVM                  7   8  70   5\n",
       "Random Forest        9   2  76   3\n",
       "Gradient Boosting    5   3  75   7\n",
       "XGBoost              7   3  75   5\n",
       "Dummy Most Frequent  0   0  78  12\n",
       "Dummy Stratified     8  40  38   4"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aux\n",
    "data = df_cm_ove\n",
    "models_objects = [lre_ove,svm_ove,rfc_ove,bst_ove,xgb_ove,dmf_ove,dst_ove]\n",
    "# Loop\n",
    "for model_name, models_object in zip(idx_models, models_objects):\n",
    "    tn, fp, fn, tp = generate_report(models_object,x_test, y_test)\n",
    "    data.loc[model_name, 'TP'] = tp\n",
    "    data.loc[model_name, 'FP'] = fp\n",
    "    data.loc[model_name, 'TN'] = tn\n",
    "    data.loc[model_name, 'FN'] = fn\n",
    "# Show results\n",
    "df_cm_ove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussions\n",
    "\n",
    "Considering only Logistic Regression for Imbalanced data and Random Forest for Oversampled data we can see that **SMOTE** increase models results.\n",
    "\n",
    "Besides that Logistic Regression presents a high rate of FP (If a pacient don't present COVID-19 and the model says that he's infected), that implies a confused model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Backtest evaluation on ML models using bootstrap CI**\n",
    "\n",
    "Will be evaluated only Logistic Regression (imbalanced) and RF (oversampled)\n",
    "\n",
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables fixed for bootstrap in backtest\n",
    "SAMPLE_SIZE = len(x_resp)\n",
    "REPETITIONS = 2000\n",
    "RS_GENERATOR = range(0,REPETITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_resampling_backtest(x_backtest, rs_number, sample_size=SAMPLE_SIZE):\n",
    "    # Generate X sample\n",
    "    bootstrap_x = resample(x_backtest, replace=True, n_samples=sample_size, random_state=rs_number)\n",
    "    # Return\n",
    "    return bootstrap_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_backtest(model, x_sample, y_sample_true):\n",
    "    # Generate a prediction using the model\n",
    "    y_sample_pred = model.predict(x_sample)\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_sample_true, y_sample_pred)\n",
    "    # Return\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backtest_metrics_using_bootstrap(model, x_backtest, y_backtest, rs_generator):\n",
    "    # Metrics list\n",
    "    acc_list = []\n",
    "    # Loop to generate a sample and generate metrics\n",
    "    for rs in rs_generator:\n",
    "        x_sample = bootstrap_resampling_backtest(x_backtest, rs_number=rs)\n",
    "        acc = calculate_metrics_backtest(model, x_sample, y_backtest)\n",
    "        # Append results\n",
    "        acc_list.append(acc)\n",
    "    # Return\n",
    "    return acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate for Selected Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre_imb_bt_results = get_backtest_metrics_using_bootstrap(lre_imb, x_resp, y_resp, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_ove_bt_results = get_backtest_metrics_using_bootstrap(rfc_ove, x_resp, y_resp, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifiy Normality to decide a comparative test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "stats.shapiro(lre_imb_bt_results)[1] > 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "stats.shapiro(rfc_ove_bt_results)[1] > 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, all results **does not follow a normal distribution** (p-value << 0.05), so it's necessary to compare then with a non-parametric test called Kruskal-Wallis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pvalue(pval):\n",
    "    print('p-value:',pval)\n",
    "    if pval > 0.05:\n",
    "        print('No significant difference between distributions (fail to reject H0)')\n",
    "    else:\n",
    "        print('Different distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 4.052444432416334e-250\n",
      "Different distributions (reject H0)\n"
     ]
    }
   ],
   "source": [
    "# Eval the comparison\n",
    "eval_pvalue(stats.kruskal(lre_imb_bt_results, rfc_ove_bt_results)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, there is difference between Logistic regression and RF results. To select a model, let's get the best mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.848"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the accuracy mean - LR\n",
    "np.mean(lre_imb_bt_results).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the accuracy mean - RF\n",
    "np.mean(rfc_ove_bt_results).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking the accuracy mean, RF have the best result and so, the best model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model overall results\n",
    "\n",
    "Calculate BCI for several metrics for selected model\n",
    "\n",
    "## Auxiliar variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables fixed for bootstrap\n",
    "REPETITIONS = 2000\n",
    "SAMPLE_SIZE = len(y_test)\n",
    "RS_GENERATOR = range(0,REPETITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_best_model(model, x_sample, y_sample_true):\n",
    "    # Generate a prediction using the model\n",
    "    y_sample_pred = model.predict(x_sample)\n",
    "    \n",
    "    # Calculate a confusion matrix to retrieve the binary CM values\n",
    "    tn, fp, fn, tp = confusion_matrix(y_sample_true, y_sample_pred).ravel()\n",
    "    \n",
    "    # Calculate several metrics with SKlearn\n",
    "    f1score = f1_score(y_sample_true, y_sample_pred)\n",
    "    roc_auc = roc_auc_score(y_sample_true, y_sample_pred)\n",
    "    recall = recall_score(y_sample_true, y_sample_pred)\n",
    "    precision = precision_score(y_sample_true, y_sample_pred)\n",
    "    \n",
    "    # Calculate others metrics manually with CM values\n",
    "    specificity = tn/(tn+fp)\n",
    "    npv = tn/(tn+fn)\n",
    "    \n",
    "    # Return\n",
    "    return f1score,roc_auc,recall,precision,specificity,npv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_using_bootstrap_best_model(model, x_testset, y_testset, rs_generator):\n",
    "    # Metrics list\n",
    "    f1s_list = []\n",
    "    roc_list = []\n",
    "    rec_list = []\n",
    "    pre_list = []\n",
    "    spe_list = []\n",
    "    npv_list = []\n",
    "    \n",
    "    # Loop to generate a sample and generate metrics\n",
    "    for rs in rs_generator:\n",
    "        x_sample, y_sample = bootstrap_resampling(x_testset, y_testset, rs_number=rs)\n",
    "        f1s,roc,rec,pre,spe,npv = calculate_metrics_best_model(model, x_sample=x_sample, y_sample_true=y_sample)\n",
    "        # Append results\n",
    "        f1s_list.append(f1s)\n",
    "        roc_list.append(roc)\n",
    "        rec_list.append(rec)\n",
    "        pre_list.append(pre)\n",
    "        spe_list.append(spe)\n",
    "        npv_list.append(npv)\n",
    "    \n",
    "    # Return\n",
    "    return f1s_list,roc_list,rec_list,pre_list,spe_list,npv_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate\n",
    "best_model_results = get_metrics_using_bootstrap_best_model(rfc_ove, x_test, y_test, rs_generator=RS_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with the answer\n",
    "idx_models = ['Random Forest']\n",
    "col_metrics = ['F1-Score','AUC ROC','Recall (Sensitivity)','Precision','Specificity','NPV']\n",
    "# Def dataframes results\n",
    "best_result = pd.DataFrame(index=idx_models, columns=col_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Recall (Sensitivity)</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>NPV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>(0.533, 0.774, 0.941)</td>\n",
       "      <td>(0.719, 0.862, 0.988)</td>\n",
       "      <td>(0.462, 0.75, 1.0)</td>\n",
       "      <td>(0.545, 0.818, 1.0)</td>\n",
       "      <td>(0.935, 0.975, 1.0)</td>\n",
       "      <td>(0.912, 0.962, 1.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            F1-Score                AUC ROC  \\\n",
       "Random Forest  (0.533, 0.774, 0.941)  (0.719, 0.862, 0.988)   \n",
       "\n",
       "              Recall (Sensitivity)            Precision          Specificity  \\\n",
       "Random Forest   (0.462, 0.75, 1.0)  (0.545, 0.818, 1.0)  (0.935, 0.975, 1.0)   \n",
       "\n",
       "                               NPV  \n",
       "Random Forest  (0.912, 0.962, 1.0)  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get intervals\n",
    "for metric_name, metric_idx in zip(col_metrics, range(0, len(col_metrics))):\n",
    "    best_result.loc[idx_models[0], metric_name] = bootstrap_confidence_interval(best_model_results[metric_idx])\n",
    "# Show\n",
    "best_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Discussion: Best model to evaluate qualitative**\n",
    "In conclusion, the best model at the end was a Random Forest created with oversampled dataset (**rfc_oversampled**).\n",
    "\n",
    "# **Export results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap CI results - Model selection\n",
    "df_ci_ove.to_csv('./results_resultsEval/bootstrap_test_selection_oversampled.csv',sep=';',decimal='.')\n",
    "df_ci_imb.to_csv('./results_resultsEval/bootstrap_test_selection_imbalanced.csv',sep=';',decimal='.')\n",
    "# Confusion matrix results\n",
    "df_cm_ove.to_csv('./results_resultsEval/confusion_matrix_test_oversampled.csv',sep=';',decimal='.')\n",
    "df_cm_imb.to_csv('./results_resultsEval/confusion_matrix_test_imbalanced.csv',sep=';',decimal='.')\n",
    "# Best model several metrics\n",
    "best_result.to_csv('./results_resultsEval/best_model_results_rfc_oversampled.csv',sep=';',decimal='.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
